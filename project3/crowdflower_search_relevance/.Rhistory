find.package("devtools")
library(devtools)
find.package("devtools")
install.packages("devtools")
library(devtools)
find_rtools()
install.packages("kernsmooth")
install.packages("KernSmooth")
library(KernSmooth)
source('~/.active-rstudio-document', echo=TRUE)
setwd('D:\Projects\data_science\nanodegree_data_analyst\ndgree\project3\crowdflower_search_relevance')
setwd('D:/Projects/data_science/nanodegree_data_analyst/ndgree/project3/crowdflower_search_relevance')
source('~/.active-rstudio-document', echo=TRUE)
setwd('D:\Projects\data_science\nanodegree_data_analyst\ndgree\project3\crowdflower_search_relevance')
source('~/.active-rstudio-document', echo=TRUE)
geom_histogram()
ggplot(aes(x = words), data = top_words) +
geom_histogram()
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(top_words)
View(word_counts)
View(top_words)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
# Hidden notes
# Set the directory of your project here:
setwd('D:/Projects/data_science/nanodegree_data_analyst/ndgree/project3/crowdflower_search_relevance')
# Ideas:
# 1. How many times each word of search queries shown in title?
#    In descriptions?
# 2. What is the percentage of words found in title and description?
# 3. Read Google's early paper about pagerank.
# 4. How many words in description & title?
# corpus <- VCorpus(VectorSource(tolower(train_raw$product_title)))
# corpus <- tm_map(corpus, removePunctuation)
# corpus <- tm_map(corpus, removeWords, stopwords("english"))
# corpus <- tm_map(corpus, stemDocument)
# init
# readr library to successfully read csv files.
suppressWarnings(library(readr))
train_raw <- read_csv('./inputs/train.csv')
test_raw <- read_csv('./inputs/test.csv')
# ggplot2 for plotting.
suppressWarnings(library(ggplot2))
# tm for text processing.
suppressWarnings(suppressMessages(library(tm)))
# SnowballC for word stemming used by tm.
suppressWarnings(library(SnowballC))
names(test_raw)
nrow(train_raw)
unique(train_raw$product_title)[1:10]
# The number of unique product titles in the training set
length(unique(train_raw$product_title))
# The number of product titles that are only in the train set or only in the test set
length(setdiff(unique(train_raw$product_title), unique(test_raw$product_title)))
# The number of product titles that are in both the train and test sets
length(intersect(unique(train_raw$product_title), unique(test_raw$product_title)))
nrow(test_raw)
unique(train_raw$query)[1:10]
# The number of unique train queries
length(unique(train_raw$query))
# The number of unique test queries
length(unique(train_raw$query))
# Are any queries different between the sets?
length(setdiff(unique(train_raw$query), unique(test_raw$query)))
names(train_raw)
# Creating a function plot_word_counts to plot counts of word occurences in different sets
plot_word_counts <- function(documents) {
# Keep only unique documents and convert them to lowercase
corpus <- Corpus(VectorSource(tolower(unique(documents))))
# Remove punctuation from the documents
corpus <- tm_map(corpus, removePunctuation)
# Remove english stopwords, such as "the" and "a"
corpus <- tm_map(corpus, removeWords, stopwords("english"))
doc_terms <- DocumentTermMatrix(corpus)
doc_terms <- as.data.frame(as.matrix(doc_terms))
word_counts <- data.frame(Words=colnames(doc_terms), Counts=colSums(doc_terms))
# Sort from the most frequent words to the least frequent words
word_counts <- word_counts[order(word_counts$Counts, decreasing=TRUE),]
top_words <- word_counts[1:10,]
top_words$Words <- factor(top_words$Words, levels=top_words$Words)
# Plot the words.
ggplot(aes(x = Words, y = Counts, fill = "maroon"), data = top_words) +
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(train_raw)
View(train_raw)
max(train_raw$median_relevance)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
plot_word_counts <- function(documents) {
# Keep only unique documents and convert them to lowercase
corpus <- Corpus(VectorSource(tolower(unique(documents))))
# Remove punctuation from the documents
corpus <- tm_map(corpus, removePunctuation)
# Remove english stopwords, such as "the" and "a"
corpus <- tm_map(corpus, removeWords, stopwords("english"))
doc_terms <- DocumentTermMatrix(corpus)
doc_terms <- as.data.frame(as.matrix(doc_terms))
word_counts <- data.frame(Words=colnames(doc_terms), Counts=colSums(doc_terms))
# Sort from the most frequent words to the least frequent words
word_counts <- word_counts[order(word_counts$Counts, decreasing=TRUE),]
top_words <- word_counts[1:10,]
top_words$Words <- factor(top_words$Words, levels=top_words$Words)
# Plot the words.
ggplot(aes(x = Words, y = Counts), data = top_words) +
geom_bar(stat = "identity") +
scale_y_continuous(breaks = seq(0,max(top_words$Counts), 1))
}
plot_word_counts(c(train_raw$query, test_raw$query))
source('~/.active-rstudio-document', echo=TRUE)
