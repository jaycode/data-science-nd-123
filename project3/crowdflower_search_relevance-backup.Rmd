---
title: "Crowdflower Search Result Relevance"
author: "Jay Teguh Wijaya"
date: "Friday, May 15, 2015"
output: html_document
---

## 1. Overview

In this project, I am visualising a dataset of a Kaggle competition called [Search Result Relevance](https://www.kaggle.com/c/crowdflower-search-relevance) by **CrowdFlower**. By choosing this dataset, I am hoping to prove the practicality of doing Udacity courses i.e. have we really learned relevant knowledge to real-world data analysis problem? and what is the gap between what we have learned so far to what the industry needs?

The dataset in this project has initially 6 variables, but we will have to preprocess it so we can use it in our analysis. In the end the dataset used in our analysis would contain 8+ variables, thus adhering to this project's requirements.

## 2. Competition Description

So many of our favorite daily activities are mediated by proprietary search algorithms. Whether you're trying to find a stream of that reality TV show on cat herding or shopping an eCommerce site for a new set of Japanese sushi knives, the relevance of search results is often responsible for your (un)happiness. Currently, small online businesses have no good way of evaluating the performance of their search algorithms, making it difficult for them to provide an exceptional customer experience.

The goal of this competition is to create an open-source model that can be used to measure the relevance of search results. In doing so, you'll be helping enable small business owners to match the experience provided by more resource rich competitors. It will also provide more established businesses a model to test against. Given the queries and resulting product descriptions from leading eCommerce sites, this competition asks you to evaluate the accuracy of their search algorithms.

## 3. About the Dataset

To evaluate search relevancy, CrowdFlower has had their crowd evaluate searches from a handful of eCommerce websites. A total of 261 search terms were generated, and CrowdFlower put together a list of products and their corresponding search terms. Each rater in the crowd was asked to give a product search term a score of 1, 2, 3, 4, with 4 indicating the item completely satisfies the search query, and 1 indicating the item doesn't match the search term.

The challenge in this competition is to predict the relevance score given the product description and product title. To ensure that your algorithm is robust enough to handle any noisy HTML snippets in the wild real world, the data provided in the product description field is raw and contains information that is irrelevant to the product.

To discourage hand-labeling the data, CrowdFlower has also provided extra data that was not labeled by the crowd in the test set. This data is ignored when calculating your score.

There are two datasets provided for this competition:

1. **train.csv**: Contains training data, which contains target responses.
2. **test.csv**: Contains test data that has no target responses.

## 3. Initial Analysis and Data Exploration

**Note**: Much of the scripts here are taken from the competition's official documentation. [^1]

Let us first load the data and include all the required libraries.

```{r echo=FALSE}
# Hidden notes
# Set the directory of your project here:
setwd('D:/Projects/data_science/nanodegree_data_analyst/ndgree/project3/crowdflower_search_relevance')
```

```{r results='hide'}
# init
# readr library to successfully read csv files.
suppressWarnings(library(readr))
train_raw <- read_csv('./inputs/train.csv')
test_raw <- read_csv('./inputs/test.csv')

# Our processed datasets
train <- data.frame(id = train_raw$id,
                    median_relevance = train_raw$median_relevance,
                    relevance_variance = train_raw$relevance_variance)
test <- data.frame(id = test_raw$id)

# ggplot2 for plotting.
suppressWarnings(library(ggplot2))
# GGally for initial quick-plotting multiple variables.
suppressWarnings(library(GGally))
# tm for text processing.
suppressWarnings(suppressMessages(library(tm)))
# SnowballC for word stemming used by tm.
suppressWarnings(library(SnowballC))
# korPus and TreeTagger for lemmaterization.
suppressWarnings(suppressMessages(library(koRpus)))
# For grid.arrange to work.
suppressWarnings(suppressMessages(library(gridExtra)))
# For fread to work.
suppressWarnings(suppressMessages(library(data.table)))
# Set the path to your local TreeTagger installation accordingly.
treetagger_path <- "C://TreeTagger"
```

See that `train_raw` has 6 columns:

```{r}
names(train_raw)
```

While `test_raw` has 4 columns:

```{r}
names(test_raw)
```

Some explanations of the columns:

- **id**: Product ID
- **query**: Search term used.
- **product_title**: Name of the product.
- **product_description**: The full product description along with HTML formatting tags.
- **median_relevance**: Median relevance score by 3 raters. This value is an integer between 1 and 4. Only available in `train_raw` dataset.
- **relevance_variance**: Variance of the relevance scores given by raters. Only available in `train_raw` dataset.

Number of rows in each datasets:

```{r}
nrow(train_raw)
```

```{r}
nrow(test_raw)
```

Let's look at the queries in the train set now.
```{r}
unique(train_raw$query)[1:10]
# The number of unique train queries
length(unique(train_raw$query))
# The number of unique test queries
length(unique(train_raw$query))
# Are any queries different between the sets?
length(setdiff(unique(train_raw$query), unique(test_raw$query)))
```
It looks like all the queries we see in the training set are also in the test set.

Now let's look at the product titles
```{r}
unique(train_raw$product_title)[1:10]
# The number of unique product titles in the training set
length(unique(train_raw$product_title))
# The number of product titles that are only in the train set or only in the test set
length(setdiff(unique(train_raw$product_title), unique(test_raw$product_title)))
# The number of product titles that are in both the train and test sets
length(intersect(unique(train_raw$product_title), unique(test_raw$product_title)))
```
This tells us that we only see most of the product titles once, and that the product titles are mostly different between the train and test sets.

Now let's start with some basic text analysis on the queries. First, we'll create a helper function

```{r}
# Creating a function plot_word_counts to plot counts of word occurences in different sets
plot_word_counts <- function(documents) {
  # Keep only unique documents and convert them to lowercase
  corpus <- Corpus(VectorSource(tolower(unique(documents))))
  # Remove punctuation from the documents
  corpus <- tm_map(corpus, removePunctuation)
  # Remove english stopwords, such as "the" and "a"
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  
  doc_terms <- DocumentTermMatrix(corpus)
  doc_terms <- as.data.frame(as.matrix(doc_terms))
  word_counts <- data.frame(Words=colnames(doc_terms), Counts=colSums(doc_terms))
  # Sort from the most frequent words to the least frequent words
  word_counts <- word_counts[order(word_counts$Counts, decreasing=TRUE),]
  
  top_words <- word_counts[1:10,]
  top_words$Words <- factor(top_words$Words, levels=top_words$Words)

  # Plot the words.
  ggplot(aes(x = Words, y = Counts), data = top_words) +
    geom_bar(stat = "identity", fill = '#4B92E3') +
    scale_y_continuous(breaks = seq(0,max(top_words$Counts), ceiling(max(top_words$Counts)/10)))
}

```

Then we will apply that function to find the most common terms in the query, product title, and product description.

```{r}
# The top words in the query
plot_word_counts(c(train_raw$query, test_raw$query))
# The top words in the product title (from a random sample for computational reasons)
set.seed(1)
plot_word_counts(sample(c(train_raw$product_title, test_raw$product_title), 1000))
# The top words in the product title (from a random sample for computational reasons)
set.seed(1)
plot_word_counts(sample(c(train_raw$product_description, test_raw$product_description), 1000))
```

## 4. Working Pipeline

I divide the working pipeline for this project into 3 parts:
1. **Discovering The Variables**: In here I would try out several preprocessing ideas to find out which variables have considerable impact on our model.
2. **Preprocessing Dataset**: In here I would preprocess our dataset based on our findings on previous step.
3. **Machine Learning Algorithms**: In this part I am going to try out several ML algorithms and benchmark them based on their performance and precision. Several submissions may be required here to find out how well the model generalise on different dataset.

For the course of this project I will only document R's linear regression, but if you are interested in more details or hook up to work on more kaggle projects together, I would be happy to share. Send me an email teguh w purwanto at gmail if you do.

## 5. The Actual Work
### 5.1. Discovering The Variables

Now that we get a little more understanding on the data, we can begin to wrangle with it to squeeze out useful variables from the dataset.

There are some preprocessing ideas that comes up directly after seeing the initial data, that may relate to relevance:

1. How many times each word of search queries shown in title? In descriptions? Say for example if a user searches for "red fishing rod", then product "blue fishing rod" is obviously more relevant than "fishing hat".
2. But in the same scenario, how about "red fishing hat"? maybe we need to divide the sentences into noun, adjective, verb, etc, and search from them first?
3. Another important question is word synonyms. It would be expected that users looking for "dark women dress" be shown results containing "black women dress" and "black women dresses". For this reason *lemmatizating* the texts is important, but not so for *stemming* them (explained below).
4. Let's review Google's early paper about search engines, we may be able to get some insights from there.
5. Let's try other unlikely variables just for fun. Does number of words in product title and description have a correlation to relevance, somehow?

#### 5.1.5. Setting Up Score Calculation Helpers

##### TF-IDF Scoring

Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.

##### Computing TF-IDF:
**TF:** `TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)`
**IDF:** `IDF(t) = log_e(Total number of documents / Number of documents with term t in it)`

Let's add this to our preprocessed dataset and see how effective it is.

```{r}
calculate_tfidf_score <- function(queries, texts) {
  scores <- list(length=length(queries))
  for (i in 1:length(queries)) {
    query <- queries[i]
    text <- texts[i]
    # Remove leading and trailing whitespaces, then split string by whitespaces.
    query_nodes <- unique(strsplit(gsub("^\\s+|\\s+$", "",
                                        gsub("\\s+", " ", query)), " ")[[1]])
    text_nodes <- unique(strsplit(gsub("^\\s+|\\s+$", "",
                                       gsub("\\s+", " ", text)), " ")[[1]])
    score <- 0
    for(query_node in query_nodes) {
      score <- score + length(grep(query_node, text_nodes))
    }
    the_score <- score / length(query_nodes)
    if(the_score > 1) {the_score <- 1}
    scores[[i]] <- (the_score)
  }
  return(as.numeric(scores))
}

# Testing function calculate_match_score
test_calculate_match_score <- function() {
  queries <- c("first query",
               "second query ",
               "third query",
               " this fourth query should",
               "led christma light",
               "soda stream")
  texts <- c(
    "first one should  return 0.5",
    "this one should return 0",
    "this third query   should return 1 third",
    "this fourth one should return 0.75",
    "set  10 batteri oper multi led train christma light  clear wire",
    "sodastream home soda maker kit")
  scores <- calculate_match_score(queries, texts)
  stopifnot(all.equal(scores[[1]], 0.5) &&
            all.equal(scores[[2]], 0) &&
            all.equal(scores[[3]], 1) &&
            all.equal(scores[[4]], 0.75) &&
            all.equal(scores[[5]], 1) &&
            all.equal(scores[[6]], 1))
}
```

```{r}
# This function calculates the score of matches between a list of queries and 
# a list of texts. We are trying different methods to calculate the score here,
# so rather than reading the method from documentation, review how it works from
# test function test_calculate_match_score (Yes, I do love TDD).
calculate_match_score <- function(queries, texts) {
  scores <- list(length=length(queries))
  for (i in 1:length(queries)) {
    query <- queries[i]
    text <- texts[i]
    # Remove leading and trailing whitespaces, then split string by whitespaces.
    query_nodes <- unique(strsplit(gsub("^\\s+|\\s+$", "",
                                        gsub("\\s+", " ", query)), " ")[[1]])
    text_nodes <- unique(strsplit(gsub("^\\s+|\\s+$", "",
                                       gsub("\\s+", " ", text)), " ")[[1]])
    score <- 0
    for(query_node in query_nodes) {
      score <- score + length(grep(query_node, text_nodes))
    }
    the_score <- score / length(query_nodes)
    if(the_score > 1) {the_score <- 1}
    scores[[i]] <- (the_score)
  }
  return(as.numeric(scores))
}

# Testing function calculate_match_score
test_calculate_match_score <- function() {
  queries <- c("first query",
               "second query ",
               "third query",
               " this fourth query should",
               "led christma light",
               "soda stream")
  texts <- c(
    "first one should  return 0.5",
    "this one should return 0",
    "this third query   should return 1 third",
    "this fourth one should return 0.75",
    "set  10 batteri oper multi led train christma light  clear wire",
    "sodastream home soda maker kit")
  scores <- calculate_match_score(queries, texts)
  stopifnot(all.equal(scores[[1]], 0.5) &&
            all.equal(scores[[2]], 0) &&
            all.equal(scores[[3]], 1) &&
            all.equal(scores[[4]], 0.75) &&
            all.equal(scores[[5]], 1) &&
            all.equal(scores[[6]], 1))
  
}
test_calculate_match_score()
train$title_simple_score <- calculate_match_score(train_raw$query, train_raw$product_title)
train$description_simple_score <- calculate_match_score(train_raw$query, train_raw$product_description)
```

Let's plot simple scoring system to median relevance and relevance variance to see how effective it is.

```{r}
p1 <- ggplot(aes(x = median_relevance, y = title_simple_score), data = train) +
  geom_jitter(alpha=.2)
p2 <- ggplot(aes(x = median_relevance, y = description_simple_score), data = train) +
  geom_jitter(alpha=.2)

grid.arrange(p1, p2, ncol = 2)
```

From our initial plot we see that:

1. As title and description simple scores get larger, so do median relevance. This shows that there is a correlation between more terms in query found within title and description.
2. There are still some missing variables here, as there are many items with 0 title and description simple scores having 4 in median relevance. In other words many people voted that the results are relevant although they see no term they enquired within title and description scores.
3. In the case of relevance variance, it seems like queries with high (1.0) title and description scores have less relevance variance than the rest. I am unclear as to what this may mean.

Before moving forward, let's see if by using stemmed words we could clarify the results further as by using stemmed words -> less words to compare -> higher scores in general -> we can spread observations with 0 scores into higher score values.

#### 5.1.4. Setting Up Stemming Helpers

Below is our helper to stem passed corpus.

```{r}
# `prepare_corpus` is a function to, well, prepare our corpus of words.
# From it we will run our stemming algorithm.
prepare_corpus <- function(df, var) {
  eval(parse(text=paste("corpus <- VCorpus(VectorSource(tolower(",
             "df$", var, ")))", sep="")))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, stripWhitespace)
}

# This function stems the texts found in "var" 
stem_corpus <- function(corpus) {
  corpus <- tm_map(corpus, stemDocument)
  
  corpus_text <- data.frame(
    text=unlist(sapply(corpus, `[`, "content")),
    stringsAsFactors=F
  )
  # "Simple" word count with regex.
  corpus_text$count <- sapply(gregexpr("\\b\\W+\\b", corpus_text$text, perl=TRUE), function(x) sum(x>0) ) + 1
  # corpus_text$count <- sapply(corpus_text$text, function(x) stri_stats_latex(x)[["Words"]])
  return(corpus_text)
}

test_stem_corpus <- function() {
  df <- data.frame(query = c('run', 'running', 'runs running', 'running shoes', 'ran', 'I am running'))
  corpus <- prepare_corpus(df, 'query')
  corpus_text <- stem_corpus(corpus)
  stopifnot(
    all.equal(corpus_text$text[1], 'run'),
    all.equal(corpus_text$count[1], 1),
    all.equal(corpus_text$text[2], 'run'),
    all.equal(corpus_text$count[2], 1),
    all.equal(corpus_text$text[3], 'run run'),
    all.equal(corpus_text$count[3], 2),
    all.equal(corpus_text$text[4], 'run shoe'),
    all.equal(corpus_text$count[4], 2),
    all.equal(corpus_text$text[5], 'ran'),
    all.equal(corpus_text$count[5], 1),
    all.equal(corpus_text$text[6], ' run'),
    all.equal(corpus_text$count[6], 1)
  )
}

test_stem_corpus()

# Let's try stemming for our dataset.

preprocess_data <- function(df, new_df) {  
  cat("processing queries\n")
  corpus <- prepare_corpus(df, "query")
  stemmed_df <- stem_corpus(corpus)
  new_df$query_stemmed <- stemmed_df$text
  new_df$query_stemmed_count <- stemmed_df$count
  cat("queries processed\n")

  cat("processing product_titles\n")
  corpus <- prepare_corpus(df, "product_title")
  stemmed_df <- stem_corpus(corpus)
  new_df$product_title_stemmed <- stemmed_df$text
  new_df$product_title_stemmed_count <- stemmed_df$count
  cat("product_titles processed\n")

  cat("processing product_descriptions\n")
  corpus <- prepare_corpus(df, "product_description")
  stemmed_df <- stem_corpus(corpus)
  new_df$product_description_stemmed <- stemmed_df$text
  new_df$product_description_stemmed_count <- stemmed_df$count
  cat("product_descriptions processed\n")
  
  #------------------------------------------------------------------------
  # Score Calculation
  #------------------------------------------------------------------------
  
  cat("calculating query vs stemmed product title match score\n")
  new_df$title_stemmed_match_score <- calculate_match_score(new_df$query_stemmed, new_df$product_title_stemmed)
  cat("query vs stemmed product title match score calculated\n")

  cat("calculating query vs stemmed product description match score\n")
  new_df$description_stemmed_match_score <- calculate_match_score(new_df$query_stemmed, new_df$product_description_stemmed)
  cat("query vs stemmed product description match score calculated\n")
  if ('median_relevance' %in% colnames(df)) {
    new_df$median_relevance <- df$median_relevance
  }
    
  if ('relevance_variance' %in% colnames(df)) {
    new_df$relevance_variance <- df$relevance_variance    
  }
  return(new_df)
}

# It takes awhile to preprocess the whole data, so we store 
# the dataset in a file if it does not exist yet, or load
# the file when it already does.
destfile <- "p_train.Rda"
if(!file.exists(destfile)) {
  train <- preprocess_data(train_raw, train)
  write.table(train,destfile)
} else {
  train <- fread(destfile, header = TRUE, skip = 0)
}

```

Now let's plot the score of stemmed matches with their relevance.

```{r}
# Making sure one more time that stemmed words do have correlation to search relevance.
p1 <- ggplot(aes(x = median_relevance, y = title_stemmed_match_score), data = train) +
  geom_jitter(alpha=.2)

p2 <- ggplot(aes(x = median_relevance, y = description_stemmed_match_score), data = train) +
  geom_jitter(alpha=.2)

grid.arrange(p1, p2, ncol = 2)
```

Let's compare these two systems (simple vs stemmed).

```{r}
p1 <- ggplot(aes(x = median_relevance, y = title_stemmed_match_score), data = train) +
  geom_jitter(alpha=.2)

p2 <- ggplot(aes(x = median_relevance, y = description_stemmed_match_score), data = train) +
  geom_jitter(alpha=.2)

p3 <- ggplot(aes(x = median_relevance, y = title_simple_score), data = train) +
  geom_jitter(alpha=.2)

p4 <- ggplot(aes(x = median_relevance, y = description_simple_score), data = train) +
  geom_jitter(alpha=.2)

grid.arrange(p1, p2, p3, p4, ncol = 2)

```

From this plot, we see that stemming the words does improve our model, as shown that with stemming, high match scores correlate closer with median relevance.


#### 5.1.2. Stemming Or Lemmatization?

**Stemming** usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.

**Lemmatization** usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the *lemma*.

If confronted with the token *saw*, stemming might return just *s*, whereas lemmatization would attempt to return either *see* or *saw* depending on whether the use of the token was as a *verb* or a *noun*. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma.

Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. [^2]

In our project, let us try both stemming and lemmatization and see which of them produces better model. Stemming algorithm is provided by R's Text Mining package (`package(tm)`) while Lemmatization algorithm is provided by korPus (`package(korPus)`) package with the help of `TreeTagger` app.

#### 5.1.5. Setting up Lemmatization Helpers

Lemmatization is a bit more complex as we need to build our own function based on package `koRpus` which uses external app called `TreeTagger`.

**Trying out Lemmatization Package**

*Before running the code bellow make sure you have TreeTagger installed in your system. The installation process should be rather straightforward (read INSTALL.txt document in their app). TreeTagger can be obtained from here: [TreeTagger website](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/)*
```{r}
tagged.results <- treetag(c('run', 'running', 'runs running', 'running shoes', 'ran', 'I am running'), treetagger="manual", format="obj",
                      TT.tknz=FALSE , lang="en",
                      TT.options=list(path=treetagger_path, preset="en"))
# When everything is correct, following code should display the lemma for given words:
tagged_results <- tagged.results@TT.res
tagged_results$token
# All the above tokens are converted into following lemma:
tagged_results$lemma
```

Once we are sure `TreeTagger` package works well we can start writing our helpers.

```{r echo=FALSE, eval=FALSE}
# Trying out transformation feature of tm package.
hash_replace <- function(x,h) {
  if (length(h[[x]])>0) {
      return(h[[x]])
  } else {
      return(x)
  }
}

replaceWords <- function(x,h) {
  y = unlist(strsplit(x$content," "))
  y=y[which(as.logical(nchar(y)))]
  z = unlist(lapply(y,hash_replace,h))
  return(PlainTextDocument(paste(unlist(z),collapse=' ')))
}


df = data.frame(query = c("Justin Bieber is a genius."))
corpus <- VCorpus(VectorSource(df$query))
replacements <- new.env( hash = TRUE, parent = emptyenv(), size = length( 2 ) )
replacements[['Justin']] <- 'Elon'
replacements[['Bieber']] <- 'Musk'
corpus <- tm_map(corpus, replaceWords, h = replacements)

corpus_text <- data.frame(
  text=unlist(sapply(corpus, `[`, "content")),
  stringsAsFactors=F
)

stopifnot(all.equal(corpus_text$text[1], "Elon Musk is a genius."))
```


```{r}
counter <<- 0
# This is an equivalent of function "stemDocument" when we did our stemming algorithm earlier.
lemmatizeDocument <- function(x, language = 'en') {
  suppressMessages(tagged.results <- treetag(content(x), treetagger="manual", format="obj",
                        TT.tknz=FALSE , lang=language,
                        TT.options=list(path=treetagger_path, preset="en")))
  # When everything is correct, following code should display the lemma for given words:
  tagged_results <- tagged.results@TT.res
  counter <<- counter + 1
  cat("count: ", counter, "\n")
  return(PlainTextDocument(paste(unlist(tagged_results$lemma),collapse=' ')))
}

lemmatize_corpus <- function(corpus) {
  corpus <- tm_map(corpus, lemmatizeDocument)
  # tagged.results <- treetag(corpus$content, treetagger="manual", format="obj",
  #                       TT.tknz=FALSE , lang=language,
  #                       TT.options=list(path=treetagger_path, preset="en"))
  tagged_results <- tagged.results@TT.res
  
  corpus_text <- data.frame(
    text=unlist(sapply(corpus, `[`, "content")),
    stringsAsFactors=F
  )
  # "Simple" word count with regex.
  corpus_text$count <- sapply(gregexpr("\\b\\W+\\b", corpus_text$text, perl=TRUE), function(x) sum(x>0) ) + 1
  return(corpus_text)
}

test_lemmatize_corpus <- function() {
  df <- data.frame(query = c('run',
                             'running',
                             'runs running',
                             'running shoes',
                             'ran',
                             'I am running'))
  df$query <- as.character(df$query)
  corpus <- prepare_corpus(df, 'query')
  # corpus_text <- lemmatizeDocument(df$query)
  corpus_text <- lemmatize_corpus(corpus)
  stopifnot(
    all.equal(corpus_text$text[1], 'run'),
    all.equal(corpus_text$count[1], 1),
    all.equal(corpus_text$text[2], 'run'),
    all.equal(corpus_text$count[2], 1),
    all.equal(corpus_text$text[3], 'run run'),
    all.equal(corpus_text$count[3], 2),
    all.equal(corpus_text$text[4], 'run shoe'),
    all.equal(corpus_text$count[4], 2),
    all.equal(corpus_text$text[5], 'run'),
    all.equal(corpus_text$count[5], 1),
    all.equal(corpus_text$text[6], 'run'),
    all.equal(corpus_text$count[6], 1)
  )
}

# test_lemmatize_corpus()
```

```{r}
quick_lemmatize <- function(x, language = 'en') {
  alltext = paste(unlist(x), collapse = " | ")
  tagged.results <- treetag(alltext, treetagger="manual", format="obj",
                        TT.tknz=FALSE , lang=language,
                        TT.options=list(path=treetagger_path, preset="en"))
  # When everything is correct, following code should display the lemma for given words:
  tagged_results <- tagged.results@TT.res
  lemma <- split(tagged_results$lemma, cumsum(tagged_results$lemma=='|'))
  lemma[[1]] = c('|', lemma[[1]])
  lemma <- lapply(lemma, function(x) tail(x, -1))

  return(lemma)
}
test_quick_lemmatizer <- function() {
  df <- data.frame(query = c('run',
                             'running',
                             'runs running',
                             'running shoes',
                             'ran',
                             'I am running'))
  df$query <- as.character(df$query)
  # corpus <- prepare_corpus(df, 'query')
  corpus_text <- quick_lemmatize(x = df$query)
  stopifnot(
    all.equal(corpus_text$text[1], 'run'),
    all.equal(corpus_text$count[1], 1),
    all.equal(corpus_text$text[2], 'run'),
    all.equal(corpus_text$count[2], 1),
    all.equal(corpus_text$text[3], 'run run'),
    all.equal(corpus_text$count[3], 2),
    all.equal(corpus_text$text[4], 'run shoe'),
    all.equal(corpus_text$count[4], 2),
    all.equal(corpus_text$text[5], 'run'),
    all.equal(corpus_text$count[5], 1),
    all.equal(corpus_text$text[6], 'run'),
    all.equal(corpus_text$count[6], 1)
  )
}

# test_quick_lemmatizer()

quick <- quick_lemmatize(x = train_raw$query)
```

Great! Seems like lemmatization works as well.


#### 5.1.6. Wrap Them Up Together

We have all the helpers we need, now it is time to use them.

```{r}
# The initial preprocess_data function. Later after we find the better variables
# rewrite this function.
preprocess_data <- function(df, new_df) {  
  cat("processing queries\n")
  corpus <- prepare_corpus(df, "query")
  stemmed_df <- stem_corpus(corpus)
  new_df$query_stemmed <- stemmed_df$text
  new_df$query_stemmed_count <- stemmed_df$count
  counter <<- 0
  lemma_df <- lemmatize_corpus(corpus)
  new_df$query_lemma <- lemma_df$text
  new_df$query_lemma_count <- lemma_df$count  
  cat("queries processed\n")

  cat("processing product_titles\n")
  corpus <- prepare_corpus(df, "product_titles")
  stemmed_df <- stem_corpus(corpus)
  new_df$product_title_stemmed <- stemmed_df$text
  new_df$product_title_stemmed_count <- stemmed_df$count
  counter <<- 0
  lemma_df <- lemmatize_corpus(corpus)
  new_df$product_title_lemma <- lemma_df$text
  new_df$product_title_lemma_count <- lemma_df$count  
  cat("product_titles processed\n")

  cat("processing product_descriptions\n")
  corpus <- prepare_corpus(df, "product_descriptions")
  stemmed_df <- stem_corpus(corpus)
  new_df$product_description_stemmed <- stemmed_df$text
  new_df$product_description_stemmed_count <- stemmed_df$count
  counter <<- 0
  lemma_df <- lemmatize_corpus(corpus)
  new_df$product_description_lemma <- lemma_df$text
  new_df$product_description_lemma_count <- lemma_df$count  
  cat("product_descriptions processed\n")
  
  #------------------------------------------------------------------------
  # Score Calculation
  #------------------------------------------------------------------------
  
  cat("calculating query vs stemmed product title match score\n")
  new_df$title_stemmed_match_score <- calculate_match_score(new_df$query_stemmed, new_df$product_title_stemmed)
  cat("query vs stemmed product title match score calculated\n")

  cat("calculating query vs stemmed product description match score\n")
  new_df$description_stemmed_match_score <- calculate_match_score(new_df$query_stemmed, new_df$product_description_stemmed)
  cat("query vs stemmed product description match score calculated\n")

    cat("calculating query vs lemmatized product title match score\n")
  new_df$title_lemma_match_score <- calculate_match_score(new_df$query_lemma, new_df$product_title_lemma)
  cat("query vs lemmatized product title match score calculated\n")

  cat("calculating query vs lemmatized product description match score\n")
  new_df$description_lemma_match_score <- calculate_match_score(new_df$query_lemma, new_df$product_description_lemma)
  cat("query vs lemmatized product description match score calculated\n")

  if ('median_relevance' %in% colnames(df)) {
    new_df$median_relevance <- df$median_relevance
  }
    
  if ('relevance_variance' %in% colnames(df)) {
    new_df$relevance_variance <- df$relevance_variance    
  }
  return(new_df)
}
```

```{r cache=TRUE, eval=FALSE}
# This code takes awhile to run.
train <- preprocess_data(train_raw, train)
save(train, file="preprocessed_train.Rda")
```

```{r eval=FALSE}
head(train)
ggplot(aes(x = median_relevance,
           y = title_match_score),
       data = train[sample(1:length(train$median_relevance), 10000), ]) +
  geom_jitter(alpha=.2)
```

#### 5.1.6. Google's Early Paper on Search Engines

In Google's early paper about pagerank chapter 3.1. and 3.2., it is explained that searching on the web is vastly different than searching on controlled environment, like text documents, for example. I wonder if we could somehow infer meta information of the products we can come up with better predictions? [^3]

### 5.2. Preprocessing Dataset

#### 5.2.1. Final Plots and Summary

### 5.3. Machine Learning Algorithms

## 6. Reflection

## 7. References

[^1]: Exploring Crowdflower Data: https://www.kaggle.com/users/993/ben-hamner/crowdflower-search-relevance/exploring-the-crowdflower-data
[^2]: TF-IDF - A Single Page Tutorial: http://www.tfidf.com/
[^3]: Graham Williams (2014) Hands-On Data Science with R Text Mining http://onepager.togaware.com/TextMiningO.pdf
[^4]: Stemming and lemmatization. http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html
[^5]: Brin, S. and Page, L. (1998) The Anatomy of a Large-Scale Hypertextual Web Search Engine. http://ilpubs.stanford.edu:8090/361/
